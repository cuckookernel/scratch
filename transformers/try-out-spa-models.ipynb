{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = pipeline( 'feature-extraction', device=0, model='mrm8488/bert-spanish-cased-finetuned-pos' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "del q_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feats( feat_list: List[List[float]] ):\n",
    "    arr = np.array( feat_list )\n",
    "    # print( arr.shape )\n",
    "    agg = arr.mean(axis=0)\n",
    "    x = agg - agg.mean()\n",
    "    sigma = np.sqrt( (x ** 2).sum() )\n",
    "    return x / sigma\n",
    "\n",
    "def avg_feats_for_each( predict_result: List[List[List[float]]]):\n",
    "    return np.vstack([avg_feats(feats) for feats in predict_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_normalized( feat : List[List[float]] ):\n",
    "    mat = np.array( feat )\n",
    "    means = mat.mean(axis=1, keepdims=True)\n",
    "    print( means.shape )\n",
    "    mat -= means\n",
    "    sigmas = np.sqrt( (mat ** 2).sum(axis=1, keepdims=True) ) \n",
    "    return mat / sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = [\n",
    "    \"\"\"Cabrera 85-12, es un proyecto de apartamentos tipo loft, ubicado en el corazón gastronómico y cultural de Bogotá. El edificio se encuentra sobre la calle 85 rodeado de restaurantes, centros comerciales, el centro financiero y vías de acceso. Los apartamentos cuentan con acabados y se entregan semi-dotados. Cuenta con zonas comunes como terraza con TRX, salón de trabajo, lavandería y parqueaderos.\"\"\",\n",
    "    \"DEC0 120 es un proyecto ubicado en una de las mejores y más exclusivas zonas de la ciudad, rodeado de parques y calles comerciales, es el lugar perfecto para vivir. Con excelentes vías de acceso y los mejores restaurantes y comercios a solo pasos del proyecto, hacen de este edificio con acabados de lujo, una hermosa terraza y apartamentos de 1 y 2 habitaciones uno de los proyectos mas destacados del sector.\".replace(' 2 ', ' dos ')\n",
    "]\n",
    "\n",
    "qrys = [ \"apartamentos tipo loft\",\n",
    "         \"apartamentos de dos habitaciones\",\n",
    "         \"apartamentos de 2 habitaciones\",\n",
    "         \"apartamentos de tres habitaciones Bogotá\",\n",
    "         \"casass de tres habitaciones\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 1)\n",
      "(84, 1)\n",
      "(7, 1)\n",
      "(7, 1)\n",
      "(7, 1)\n",
      "(7, 1)\n",
      "(7, 1)\n"
     ]
    }
   ],
   "source": [
    "desc_feats = [ to_numpy_normalized(feat) for feat in featurizer.predict( descs ) ]\n",
    "\n",
    "q_feats = [ to_numpy_normalized(feat) for feat in featurizer.predict( qrys ) ]\n",
    "\n",
    "q_mat = avg_feats_for_each( q_feats )\n",
    "\n",
    "# for feat in desc_feats: \n",
    "#    print( feat.shape )\n",
    "#    print( (feat ** 2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_mat = avg_feats_for_each( desc_feats )\n",
    "desc_norms = np.sqrt( (desc_mat * desc_mat).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.78241159e-18, 0.00000000e+00]),\n",
       " array([ 0.00000000e+00, -1.15648232e-17,  4.62592927e-18, -1.50342701e-17]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_mat.mean(axis=1), q_mat.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73820766, 0.68683912],\n",
       "       [0.78245837, 0.72678742],\n",
       "       [0.8830536 , 0.88933139],\n",
       "       [0.70350262, 0.66868772],\n",
       "       [0.70216861, 0.65906851]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot( q_mat, desc_mat.transpose() ) / q_norms.reshape(-1, 1) / desc_norms.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in feats[1]: print( len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_avg_best_match( q_feat_avg: np.array, desc_feats: np.array ):\n",
    "    return np.max( np.dot( desc_feats, q_feat_avg ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 apartamentos tipo loft\n",
      "\t 0.8272203830852722\n",
      "\t 0.8099554336580235\n",
      "1 apartamentos de dos habitaciones\n",
      "\t 0.7568476878900734\n",
      "\t 0.7667325027540492\n",
      "2 apartamentos de 2 habitaciones\n",
      "\t 0.772081891032358\n",
      "\t 0.782980245204741\n",
      "3 apartamentos de tres habitaciones Bogotá\n",
      "\t 0.7691742877510046\n",
      "\t 0.7885440435250596\n",
      "4 casass de tres habitaciones\n",
      "\t 0.7717626807417166\n",
      "\t 0.7779716321332767\n"
     ]
    }
   ],
   "source": [
    "for q_i, q in enumerate(qrys):\n",
    "    print( q_i, q ) \n",
    "    for d_i, desc_feat in enumerate(desc_feats):\n",
    "        score =  q_avg_best_match( q_mat[q_i, :], desc_feat )\n",
    "        print( '\\t', score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teo/git/scratch/venv-torch/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.15911121666431427, 'start': 8, 'end': 13, 'answer': 'desde'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier( {'question': 'tienen aptos con balcon?', \n",
    "             'context': 'precios desde $400, el area es 20 m2. la haus es una inmobiliaria digital. todo lo que quieras'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pipeline in module transformers.pipelines:\n",
      "\n",
      "pipeline(task: str, model: Optional = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, framework: Union[str, NoneType] = None, **kwargs) -> transformers.pipelines.Pipeline\n",
      "    Utility factory method to build a :class:`~transformers.Pipeline`.\n",
      "    \n",
      "    Pipelines are made of:\n",
      "    \n",
      "        - A :doc:`tokenizer <tokenizer>` in charge of mapping raw textual input to token.\n",
      "        - A :doc:`model <model>` to make predictions from the inputs.\n",
      "        - Some (optional) post processing for enhancing model's output.\n",
      "    \n",
      "    Args:\n",
      "        task (:obj:`str`):\n",
      "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
      "    \n",
      "            - :obj:`\"feature-extraction\"`: will return a :class:`~transformers.FeatureExtractionPipeline`.\n",
      "            - :obj:`\"sentiment-analysis\"`: will return a :class:`~transformers.TextClassificationPipeline`.\n",
      "            - :obj:`\"ner\"`: will return a :class:`~transformers.TokenClassificationPipeline`.\n",
      "            - :obj:`\"question-answering\"`: will return a :class:`~transformers.QuestionAnsweringPipeline`.\n",
      "            - :obj:`\"fill-mask\"`: will return a :class:`~transformers.FillMaskPipeline`.\n",
      "            - :obj:`\"summarization\"`: will return a :class:`~transformers.SummarizationPipeline`.\n",
      "            - :obj:`\"translation_xx_to_yy\"`: will return a :class:`~transformers.TranslationPipeline`.\n",
      "            - :obj:`\"text-generation\"`: will return a :class:`~transformers.TextGenerationPipeline`.\n",
      "            - :obj:`\"conversation\"`: will return a :class:`~transformers.ConversationalPipeline`.\n",
      "        model (:obj:`str` or :obj:`~transformers.PreTrainedModel` or :obj:`~transformers.TFPreTrainedModel`, `optional`):\n",
      "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
      "            actual instance of a pretrained model inheriting from :class:`~transformers.PreTrainedModel` (for PyTorch)\n",
      "            or :class:`~transformers.TFPreTrainedModel` (for TensorFlow).\n",
      "    \n",
      "            If not provided, the default for the :obj:`task` will be loaded.\n",
      "        config (:obj:`str` or :obj:`~transformers.PretrainedConfig`, `optional`):\n",
      "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
      "            identifier or an actual pretrained model configuration inheriting from\n",
      "            :class:`~transformers.PretrainedConfig`.\n",
      "    \n",
      "            If not provided, the default for the :obj:`task` will be loaded.\n",
      "        tokenizer (:obj:`str` or :obj:`~transformers.PreTrainedTokenizer`, `optional`):\n",
      "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained tokenizer inheriting from\n",
      "            :class:`~transformers.PreTrainedTokenizer`.\n",
      "    \n",
      "            If not provided, the default for the :obj:`task` will be loaded.\n",
      "        framework (:obj:`str`, `optional`):\n",
      "            The framework to use, either :obj:`\"pt\"` for PyTorch or :obj:`\"tf\"` for TensorFlow. The specified framework\n",
      "            must be installed.\n",
      "    \n",
      "            If no framework is specified, will default to the one currently installed. If no framework is specified\n",
      "            and both frameworks are installed, will default to the framework of the :obj:`model`, or to PyTorch if no\n",
      "            model is provided.\n",
      "        kwargs:\n",
      "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
      "            corresponding pipeline class for possible values).\n",
      "    \n",
      "    Returns:\n",
      "        :class:`~transformers.Pipeline`: A suitable pipeline for the task.\n",
      "    \n",
      "    Examples::\n",
      "    \n",
      "        >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
      "    \n",
      "        >>> # Sentiment analysis pipeline\n",
      "        >>> pipeline('sentiment-analysis')\n",
      "    \n",
      "        >>> # Question answering pipeline, specifying the checkpoint identifier\n",
      "        >>> pipeline('question-answering', model='distilbert-base-cased-distilled-squad', tokenizer='bert-base-cased')\n",
      "    \n",
      "        >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
      "        >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      "        >>> pipeline('ner', model=model, tokenizer=tokenizer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help( pipeline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained( model ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', '[UNK]', 'm', 'in', 'lo', '##ve']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize( \"I'm in love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9600155353546143}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(['I love you', 'we hope you don''t hate it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = pipeline('feature-extraction', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2736629545688629,\n",
       " 0.17628303170204163,\n",
       " -0.05449565127491951,\n",
       " -0.22952410578727722,\n",
       " -0.24320414662361145,\n",
       " -0.10173299908638,\n",
       " 0.3366072475910187,\n",
       " -0.0261160247027874,\n",
       " 0.04735410585999489,\n",
       " -1.099527359008789,\n",
       " -0.2792090177536011,\n",
       " 0.11661259829998016,\n",
       " -0.13834711909294128,\n",
       " 0.034527137875556946,\n",
       " -0.46139174699783325,\n",
       " 0.08165648579597473,\n",
       " 0.0911460667848587,\n",
       " 0.10929464548826218,\n",
       " -0.02681615948677063,\n",
       " -0.2182285487651825,\n",
       " 0.07168041914701462,\n",
       " -0.3322373330593109,\n",
       " 0.5394918322563171,\n",
       " -0.20307424664497375,\n",
       " 0.11105145514011383,\n",
       " 0.07048407942056656,\n",
       " 0.26183366775512695,\n",
       " 0.18038900196552277,\n",
       " -0.22586673498153687,\n",
       " 0.34462854266166687,\n",
       " 0.16476604342460632,\n",
       " 0.10727236419916153,\n",
       " -0.023726899176836014,\n",
       " -0.11883237957954407,\n",
       " -0.14308322966098785,\n",
       " 0.150472491979599,\n",
       " -0.012073159217834473,\n",
       " -0.23489944636821747,\n",
       " -0.07982596009969711,\n",
       " -0.11661696434020996,\n",
       " -0.4239788353443146,\n",
       " 0.12952463328838348,\n",
       " 0.5323837399482727,\n",
       " -0.041764579713344574,\n",
       " 0.14296166598796844,\n",
       " -0.5136411786079407,\n",
       " 0.07470597326755524,\n",
       " 0.0016788666835054755,\n",
       " -0.23290783166885376,\n",
       " 0.21812331676483154,\n",
       " 0.12992802262306213,\n",
       " 0.08135203272104263,\n",
       " -0.29499882459640503,\n",
       " 0.04833267629146576,\n",
       " 0.18773984909057617,\n",
       " 0.18742318451404572,\n",
       " -0.29545092582702637,\n",
       " 0.10080786049365997,\n",
       " -0.6258845329284668,\n",
       " 0.3776054382324219,\n",
       " -0.011277984827756882,\n",
       " 0.2614250183105469,\n",
       " 0.20650868117809296,\n",
       " -0.11918526142835617,\n",
       " -0.23590117692947388,\n",
       " 0.10744307935237885,\n",
       " 0.0487837977707386,\n",
       " 0.3179326057434082,\n",
       " -0.042689476162195206,\n",
       " -0.17770719528198242,\n",
       " -0.09921803325414658,\n",
       " 0.3990058898925781,\n",
       " 0.1973375827074051,\n",
       " 1.0902129411697388,\n",
       " 0.1793123036623001,\n",
       " -0.2247726321220398,\n",
       " 0.32444486021995544,\n",
       " -0.09763281792402267,\n",
       " 0.1737479716539383,\n",
       " 0.030511269345879555,\n",
       " 0.13921388983726501,\n",
       " 0.17919684946537018,\n",
       " 0.07821739464998245,\n",
       " -0.09794451296329498,\n",
       " -0.16074007749557495,\n",
       " -0.2928450107574463,\n",
       " 0.06050488352775574,\n",
       " -0.014576821587979794,\n",
       " -0.19182682037353516,\n",
       " 0.07946121692657471,\n",
       " 0.3146304190158844,\n",
       " -0.13781112432479858,\n",
       " -0.26065191626548767,\n",
       " 0.029404455795884132,\n",
       " -0.030834490433335304,\n",
       " 0.24772782623767853,\n",
       " -0.06021024286746979,\n",
       " 0.11827557533979416,\n",
       " 6.057263374328613,\n",
       " -0.313681036233902,\n",
       " -0.2449653297662735,\n",
       " 0.08178188651800156,\n",
       " 0.2009248286485672,\n",
       " -0.17299784719944,\n",
       " 0.17945073544979095,\n",
       " -0.5115980505943298,\n",
       " -0.08286362141370773,\n",
       " -0.3134489059448242,\n",
       " 0.09245538711547852,\n",
       " 0.4072035551071167,\n",
       " 0.37901541590690613,\n",
       " -0.1003027930855751,\n",
       " -0.04858175665140152,\n",
       " 0.017580023035407066,\n",
       " -0.0837949886918068,\n",
       " -0.35148385167121887,\n",
       " 0.14777852594852448,\n",
       " 0.07374453544616699,\n",
       " 0.2544368505477905,\n",
       " -0.2152930051088333,\n",
       " 0.2091122269630432,\n",
       " -0.0630842074751854,\n",
       " 0.9281827807426453,\n",
       " 0.22611109912395477,\n",
       " -0.14092597365379333,\n",
       " -0.1302393078804016,\n",
       " -0.05398174747824669,\n",
       " -0.3254237174987793,\n",
       " 0.16799017786979675,\n",
       " -0.19241955876350403,\n",
       " -0.5784084796905518,\n",
       " -0.2056950330734253,\n",
       " -0.13893941044807434,\n",
       " 0.064663365483284,\n",
       " -0.09540431201457977,\n",
       " 0.19421854615211487,\n",
       " -0.056051142513751984,\n",
       " -0.059914860874414444,\n",
       " -0.9067599773406982,\n",
       " -0.056661207228899,\n",
       " 0.2027679979801178,\n",
       " -0.19926802814006805,\n",
       " 0.1664508432149887,\n",
       " -0.1226220577955246,\n",
       " -0.3956986367702484,\n",
       " 2.779350996017456,\n",
       " -0.1878885179758072,\n",
       " -0.0884077399969101,\n",
       " -0.1448163539171219,\n",
       " 0.13179828226566315,\n",
       " 0.11197388917207718,\n",
       " -0.3414251506328583,\n",
       " -0.15665166079998016,\n",
       " 0.24376174807548523,\n",
       " -0.17128269374370575,\n",
       " -0.05748243257403374,\n",
       " 0.15299776196479797,\n",
       " 0.17434632778167725,\n",
       " 0.11142560094594955,\n",
       " -0.10631956160068512,\n",
       " -0.9780192375183105,\n",
       " 0.12763053178787231,\n",
       " -0.30546408891677856,\n",
       " 0.4682009220123291,\n",
       " -0.056482695043087006,\n",
       " -0.17062854766845703,\n",
       " 0.12645317614078522,\n",
       " -0.6500113010406494,\n",
       " 0.12266113609075546,\n",
       " 0.21823245286941528,\n",
       " 0.0485699288547039,\n",
       " 0.12028668820858002,\n",
       " -2.4073777198791504,\n",
       " 0.4818688631057739,\n",
       " 0.0755109041929245,\n",
       " 0.2414933294057846,\n",
       " 0.08699145913124084,\n",
       " 0.10870762169361115,\n",
       " -0.18178655207157135,\n",
       " -0.39127612113952637,\n",
       " -0.07797057181596756,\n",
       " 0.26486313343048096,\n",
       " 0.10637660324573517,\n",
       " 0.21075718104839325,\n",
       " -0.28798872232437134,\n",
       " 0.3224741220474243,\n",
       " 0.28125646710395813,\n",
       " -0.10051289945840836,\n",
       " 0.1747245192527771,\n",
       " 0.17735663056373596,\n",
       " 0.10236317664384842,\n",
       " -0.2899150550365448,\n",
       " 0.02394358441233635,\n",
       " -0.037184055894613266,\n",
       " 0.4028564691543579,\n",
       " 0.13186992704868317,\n",
       " 0.005911814980208874,\n",
       " -0.08705900609493256,\n",
       " 0.004476852715015411,\n",
       " 0.1081971675157547,\n",
       " -0.24925357103347778,\n",
       " -0.2546818256378174,\n",
       " -0.37918153405189514,\n",
       " 0.17403577268123627,\n",
       " 0.4745287597179413,\n",
       " 0.02117590606212616,\n",
       " -0.09503408521413803,\n",
       " 0.06612123548984528,\n",
       " 0.12916840612888336,\n",
       " -0.16693684458732605,\n",
       " -0.169065460562706,\n",
       " 0.012088032439351082,\n",
       " 0.19326864182949066,\n",
       " 0.3519887924194336,\n",
       " 0.5854452848434448,\n",
       " -0.3345523178577423,\n",
       " -0.018428554758429527,\n",
       " 0.17208902537822723,\n",
       " 0.06214963644742966,\n",
       " 0.09394053369760513,\n",
       " -0.3431836664676666,\n",
       " 0.011849815025925636,\n",
       " -0.30425751209259033,\n",
       " -0.11067070066928864,\n",
       " -0.10281532257795334,\n",
       " 0.08313887566328049,\n",
       " 0.1942771077156067,\n",
       " 0.2542644441127777,\n",
       " 0.045903608202934265,\n",
       " -0.3309131860733032,\n",
       " 0.3021882474422455,\n",
       " 0.25879546999931335,\n",
       " 0.2041780650615692,\n",
       " -0.064757339656353,\n",
       " -0.2911628186702728,\n",
       " 0.12396183609962463,\n",
       " -0.06299906969070435,\n",
       " 0.06176063418388367,\n",
       " -0.05445774272084236,\n",
       " 0.023921575397253036,\n",
       " -0.014758637174963951,\n",
       " -0.1737557202577591,\n",
       " 0.313433974981308,\n",
       " -0.06503309309482574,\n",
       " 0.028057990595698357,\n",
       " 0.10374228656291962,\n",
       " 0.3346635699272156,\n",
       " 0.22389352321624756,\n",
       " -0.0004586289869621396,\n",
       " 0.10023628920316696,\n",
       " 0.6212809681892395,\n",
       " 0.16629227995872498,\n",
       " -0.685276985168457,\n",
       " -0.27479681372642517,\n",
       " -0.298086553812027,\n",
       " -0.020732130855321884,\n",
       " -0.0071661993861198425,\n",
       " -1.1717549562454224,\n",
       " -0.11709767580032349,\n",
       " -0.16380757093429565,\n",
       " 0.08458837121725082,\n",
       " -3.055295944213867,\n",
       " 0.07369576394557953,\n",
       " 0.19895969331264496,\n",
       " 0.14628718793392181,\n",
       " -0.1899057775735855,\n",
       " 0.4128095507621765,\n",
       " 0.08961557596921921,\n",
       " -0.5041034817695618,\n",
       " 0.16920074820518494,\n",
       " -0.46210724115371704,\n",
       " -0.060498520731925964,\n",
       " -0.6344118714332581,\n",
       " 0.24131061136722565,\n",
       " -0.045814014971256256,\n",
       " 0.1499946266412735,\n",
       " 0.07639511674642563,\n",
       " 0.02601901814341545,\n",
       " -0.32593029737472534,\n",
       " -0.21187415719032288,\n",
       " -0.43471401929855347,\n",
       " 0.08435922116041183,\n",
       " -0.32632747292518616,\n",
       " 0.3724989593029022,\n",
       " -0.3754929006099701,\n",
       " 0.4384089708328247,\n",
       " 0.27107417583465576,\n",
       " 0.3027084767818451,\n",
       " 0.10073366016149521,\n",
       " 3.922295331954956,\n",
       " 0.5446199774742126,\n",
       " -0.29852354526519775,\n",
       " 0.08634956181049347,\n",
       " -0.37154483795166016,\n",
       " -0.07954317331314087,\n",
       " -0.1831861287355423,\n",
       " -0.4568830132484436,\n",
       " 0.016847291961312294,\n",
       " 0.030729830265045166,\n",
       " 0.010319909080862999,\n",
       " -0.25442832708358765,\n",
       " 0.14407095313072205,\n",
       " -0.6207556128501892,\n",
       " -0.03823108598589897,\n",
       " -0.46588456630706787,\n",
       " 0.08522558212280273,\n",
       " 0.05134604126214981,\n",
       " 0.09659650921821594,\n",
       " -0.49076130986213684,\n",
       " -0.027272982522845268,\n",
       " 0.11904826760292053,\n",
       " 0.06913551688194275,\n",
       " 0.12322446703910828,\n",
       " -0.03266393765807152,\n",
       " -0.08138823509216309,\n",
       " -0.26500049233436584,\n",
       " -0.4157215356826782,\n",
       " 0.4404357969760895,\n",
       " -0.2300146520137787,\n",
       " -0.9636073112487793,\n",
       " 0.28642743825912476,\n",
       " 0.19825151562690735,\n",
       " 0.0786239355802536,\n",
       " -0.22683469951152802,\n",
       " 0.11961427330970764,\n",
       " -0.10788772255182266,\n",
       " -0.10901989042758942,\n",
       " -0.302897572517395,\n",
       " -0.12138911336660385,\n",
       " -0.04449545592069626,\n",
       " -0.08238726854324341,\n",
       " 0.034109778702259064,\n",
       " -0.5307143926620483,\n",
       " -0.21569028496742249,\n",
       " -0.22857430577278137,\n",
       " -0.20210452377796173,\n",
       " -0.3130231201648712,\n",
       " 0.2017456591129303,\n",
       " -0.10345567762851715,\n",
       " 0.056600216776132584,\n",
       " -0.36373940110206604,\n",
       " -0.1617095172405243,\n",
       " -0.0306157898157835,\n",
       " 0.10629258304834366,\n",
       " 0.3883614242076874,\n",
       " 0.04190077260136604,\n",
       " 0.28541314601898193,\n",
       " -0.027624208480119705,\n",
       " 0.220259428024292,\n",
       " -0.007934615015983582,\n",
       " 0.14026202261447906,\n",
       " 0.036392584443092346,\n",
       " 0.02930140122771263,\n",
       " 0.17933250963687897,\n",
       " -0.08592713624238968,\n",
       " -0.10739611089229584,\n",
       " -0.09400074183940887,\n",
       " -0.13269764184951782,\n",
       " -0.1113637238740921,\n",
       " 0.09625257551670074,\n",
       " 0.039014704525470734,\n",
       " -2.1307647228240967,\n",
       " 0.31651875376701355,\n",
       " 0.1383928507566452,\n",
       " -0.10460800677537918,\n",
       " 0.0788852795958519,\n",
       " -0.1062006875872612,\n",
       " 0.24989962577819824,\n",
       " -0.16602599620819092,\n",
       " 0.39731281995773315,\n",
       " 0.1729092299938202,\n",
       " 0.1643851399421692,\n",
       " 0.3443480134010315,\n",
       " -0.05224807187914848,\n",
       " -0.1879127472639084,\n",
       " 0.07220045477151871,\n",
       " -0.015917815268039703,\n",
       " 0.692812442779541,\n",
       " 0.08186725527048111,\n",
       " -0.4173239469528198,\n",
       " -0.4709845185279846,\n",
       " 0.03899933770298958,\n",
       " 0.3566737174987793,\n",
       " -0.148911252617836,\n",
       " -0.018571944907307625,\n",
       " 0.1489068567752838,\n",
       " -0.012188996188342571,\n",
       " -0.38145947456359863,\n",
       " 0.015903916209936142,\n",
       " -0.04485442489385605,\n",
       " 0.2966468036174774,\n",
       " -0.09277689456939697,\n",
       " -0.13069258630275726,\n",
       " -0.29487475752830505,\n",
       " -0.16066884994506836,\n",
       " -0.04400675743818283,\n",
       " -0.07709131389856339,\n",
       " 0.2118234485387802,\n",
       " 0.32804182171821594,\n",
       " -0.2524200975894928,\n",
       " -0.08828902244567871,\n",
       " -0.136102557182312,\n",
       " -0.14842499792575836,\n",
       " 0.10075019299983978,\n",
       " -0.3610832393169403,\n",
       " -0.06384012848138809,\n",
       " -0.03280524164438248,\n",
       " -0.11437181383371353,\n",
       " -1.4735487699508667,\n",
       " -0.17919021844863892,\n",
       " -0.1541418880224228,\n",
       " -0.31486696004867554,\n",
       " -0.13909685611724854,\n",
       " 0.32461610436439514,\n",
       " -0.06103896349668503,\n",
       " 0.08863842487335205,\n",
       " 0.16653911769390106,\n",
       " 0.06367720663547516,\n",
       " 0.3856227397918701,\n",
       " 0.4542151987552643,\n",
       " 0.19447891414165497,\n",
       " -0.17511525750160217,\n",
       " -0.08843396604061127,\n",
       " -0.3225024938583374,\n",
       " -0.03205471113324165,\n",
       " 0.21684961020946503,\n",
       " -0.027204321697354317,\n",
       " -0.15693841874599457,\n",
       " 0.06712952256202698,\n",
       " 0.1325220912694931,\n",
       " -0.495739221572876,\n",
       " -0.03679322823882103,\n",
       " -0.04961933568120003,\n",
       " -0.3973224461078644,\n",
       " -0.039784934371709824,\n",
       " -0.3091064989566803,\n",
       " -0.05066939815878868,\n",
       " -0.22542144358158112,\n",
       " 0.22420695424079895,\n",
       " 5.175868988037109,\n",
       " -0.4222077429294586,\n",
       " 0.08570952713489532,\n",
       " -0.03631408140063286,\n",
       " -0.3228450417518616,\n",
       " -0.11518359929323196,\n",
       " 0.16853943467140198,\n",
       " 0.0941896066069603,\n",
       " 0.6273341178894043,\n",
       " 0.09224013984203339,\n",
       " -0.21372096240520477,\n",
       " -0.2661491334438324,\n",
       " 0.07778050750494003,\n",
       " 0.12067614495754242,\n",
       " -0.44407713413238525,\n",
       " 0.49750497937202454,\n",
       " -0.14003124833106995,\n",
       " 0.11564502865076065,\n",
       " 0.0161614827811718,\n",
       " -0.19442354142665863,\n",
       " -0.08016306161880493,\n",
       " 0.12306515127420425,\n",
       " 0.04477330669760704,\n",
       " -0.23669041693210602,\n",
       " 0.2996003329753876,\n",
       " 0.2306768149137497,\n",
       " 0.03465546295046806,\n",
       " -0.16498365998268127,\n",
       " -0.13525816798210144,\n",
       " -0.28758397698402405,\n",
       " -0.30104273557662964,\n",
       " 0.15239937603473663,\n",
       " 0.3344299793243408,\n",
       " 0.22968870401382446,\n",
       " 0.18722547590732574,\n",
       " -0.1940254122018814,\n",
       " -0.03115100972354412,\n",
       " 0.17802922427654266,\n",
       " 0.321836918592453,\n",
       " 0.18080809712409973,\n",
       " 0.2592970132827759,\n",
       " 0.03586951270699501,\n",
       " 0.9788391590118408,\n",
       " -0.0792958065867424,\n",
       " 0.2029539793729782,\n",
       " 0.1578414887189865,\n",
       " 0.08025757968425751,\n",
       " -0.020393775776028633,\n",
       " 0.36174294352531433,\n",
       " 0.2019059658050537,\n",
       " -0.24061636626720428,\n",
       " 0.2455076277256012,\n",
       " 0.33999302983283997,\n",
       " -0.22490976750850677,\n",
       " 0.02618803270161152,\n",
       " 0.15904748439788818,\n",
       " -0.06135953217744827,\n",
       " -0.18581034243106842,\n",
       " -0.23073701560497284,\n",
       " -0.12050589174032211,\n",
       " -0.10610559582710266,\n",
       " -0.10081950575113297,\n",
       " -0.008165362291038036,\n",
       " -0.2176441252231598,\n",
       " 0.10880354046821594,\n",
       " -0.3426644206047058,\n",
       " 0.25707998871803284,\n",
       " -0.18099091947078705,\n",
       " -0.250213086605072,\n",
       " -0.18221120536327362,\n",
       " -0.245136559009552,\n",
       " -0.06313872337341309,\n",
       " -0.5808247923851013,\n",
       " 0.217324361205101,\n",
       " 0.2731872797012329,\n",
       " 0.0007613666239194572,\n",
       " -0.4471861720085144,\n",
       " -0.13281720876693726,\n",
       " -0.024511730298399925,\n",
       " -0.053063567727804184,\n",
       " -0.1469593644142151,\n",
       " -0.20267359912395477,\n",
       " -0.12816235423088074,\n",
       " -0.2789120376110077,\n",
       " 0.005937186069786549,\n",
       " 0.08647804707288742,\n",
       " -0.32389187812805176,\n",
       " -0.5511491894721985,\n",
       " -0.021221404895186424,\n",
       " 0.16905418038368225,\n",
       " -0.024188969284296036,\n",
       " -0.20012535154819489,\n",
       " 0.0068619949743151665,\n",
       " -0.13072650134563446,\n",
       " -0.06878402829170227,\n",
       " 0.1813943088054657,\n",
       " -0.08265998214483261,\n",
       " -0.1258353292942047,\n",
       " -0.21930883824825287,\n",
       " 0.1017666608095169,\n",
       " 0.3246628940105438,\n",
       " 0.13566040992736816,\n",
       " 0.001564480597153306,\n",
       " 0.22359693050384521,\n",
       " 0.07539471983909607,\n",
       " -0.0546175092458725,\n",
       " 0.12365808337926865,\n",
       " 0.07697988301515579,\n",
       " -0.03225056082010269,\n",
       " 0.0073190429247915745,\n",
       " 0.20328350365161896,\n",
       " 0.07561787217855453,\n",
       " 0.2783763110637665,\n",
       " -0.24859638512134552,\n",
       " -0.1599699705839157,\n",
       " 0.07584603130817413,\n",
       " -0.0462326817214489,\n",
       " -0.28241589665412903,\n",
       " -7.046915054321289,\n",
       " 0.43550002574920654,\n",
       " 0.1544903963804245,\n",
       " 0.4190647304058075,\n",
       " -0.305986613035202,\n",
       " -0.1623680144548416,\n",
       " 0.31420812010765076,\n",
       " 0.2065909504890442,\n",
       " 0.3142945170402527,\n",
       " 0.0677897110581398,\n",
       " -0.4665463864803314,\n",
       " -0.31294479966163635,\n",
       " -1.8969720602035522,\n",
       " 0.18443502485752106,\n",
       " -0.1023714616894722,\n",
       " 0.007362821605056524,\n",
       " -0.06847909837961197,\n",
       " -0.7396076917648315,\n",
       " 0.11065372824668884,\n",
       " 0.14275889098644257,\n",
       " -0.18086321651935577,\n",
       " 0.31656163930892944,\n",
       " 0.22699402272701263,\n",
       " 0.295154333114624,\n",
       " 0.01624043844640255,\n",
       " 0.08390409499406815,\n",
       " -0.09897597134113312,\n",
       " 0.09368056803941727,\n",
       " 0.16425490379333496,\n",
       " -0.07698015123605728,\n",
       " 0.03249179944396019,\n",
       " -0.06276366114616394,\n",
       " 0.45800814032554626,\n",
       " -0.021872829645872116,\n",
       " 0.0295431986451149,\n",
       " -0.336216539144516,\n",
       " 0.028735851868987083,\n",
       " -0.1879911571741104,\n",
       " 0.2821783423423767,\n",
       " -0.3986875116825104,\n",
       " 0.0834086686372757,\n",
       " 0.02556651644408703,\n",
       " 0.31434670090675354,\n",
       " 0.15015484392642975,\n",
       " -0.39003321528434753,\n",
       " -0.18683411180973053,\n",
       " -0.5809401273727417,\n",
       " -2.223483085632324,\n",
       " 0.21999090909957886,\n",
       " 0.10397141426801682,\n",
       " 0.30309662222862244,\n",
       " 0.17518073320388794,\n",
       " 0.12117991596460342,\n",
       " 0.02164481207728386,\n",
       " -0.18215003609657288,\n",
       " 0.1489339917898178,\n",
       " 0.03341156616806984,\n",
       " -0.14984656870365143,\n",
       " 0.3913656175136566,\n",
       " -0.21350818872451782,\n",
       " -0.26256263256073,\n",
       " -0.03131251409649849,\n",
       " 0.5517008900642395,\n",
       " 0.09809066355228424,\n",
       " -0.2296515554189682,\n",
       " 0.09975634515285492,\n",
       " -0.3907190263271332,\n",
       " 0.01981719769537449,\n",
       " -0.09760858118534088,\n",
       " -0.3377738296985626,\n",
       " -0.19346782565116882,\n",
       " 0.0361563004553318,\n",
       " -0.040332552045583725,\n",
       " 0.08709602057933807,\n",
       " 0.11932109296321869,\n",
       " 0.39809292554855347,\n",
       " -0.1220654547214508,\n",
       " 0.06781978905200958,\n",
       " -0.16439861059188843,\n",
       " -0.03058546781539917,\n",
       " -0.3309015929698944,\n",
       " -0.06367972493171692,\n",
       " 0.40133562684059143,\n",
       " 0.3015468716621399,\n",
       " 0.13240498304367065,\n",
       " -0.0795973613858223,\n",
       " -0.3540097773075104,\n",
       " 0.25613272190093994,\n",
       " -0.011147579178214073,\n",
       " -0.057625848799943924,\n",
       " -0.14585842192173004,\n",
       " -0.1716693788766861,\n",
       " 0.004006748553365469,\n",
       " -0.00951382052153349,\n",
       " -0.2870617210865021,\n",
       " 0.3621406555175781,\n",
       " -0.2119896113872528,\n",
       " 0.1375667005777359,\n",
       " -0.3426525294780731,\n",
       " -0.23278969526290894,\n",
       " -0.21688605844974518,\n",
       " 0.007267542649060488,\n",
       " 0.3099709451198578,\n",
       " -0.36603233218193054,\n",
       " -0.05283445864915848,\n",
       " -0.3180149495601654,\n",
       " 2.259467601776123,\n",
       " 0.18952526152133942,\n",
       " -0.10449281334877014,\n",
       " -0.024131784215569496,\n",
       " -0.06687115132808685,\n",
       " 0.09567823261022568,\n",
       " -0.06584164500236511,\n",
       " 0.32507073879241943,\n",
       " 1.5603458881378174,\n",
       " 0.3281135857105255,\n",
       " -0.17013029754161835,\n",
       " 0.11685540527105331,\n",
       " 0.0656958818435669,\n",
       " -0.11913348734378815,\n",
       " 0.10291194915771484,\n",
       " 0.5289636850357056,\n",
       " -0.00024677111650817096,\n",
       " 0.15945814549922943,\n",
       " 0.10491056740283966,\n",
       " -0.05974036455154419,\n",
       " 0.30433639883995056,\n",
       " -0.17609182000160217,\n",
       " 0.16776995360851288,\n",
       " -0.10299459099769592,\n",
       " -0.08343309164047241,\n",
       " 0.07035467028617859,\n",
       " -0.27111026644706726,\n",
       " -0.07354890555143356,\n",
       " 0.2219562530517578,\n",
       " -0.47500038146972656,\n",
       " 0.18646283447742462,\n",
       " -0.043004073202610016,\n",
       " -0.06932709366083145,\n",
       " 0.9876015782356262,\n",
       " -0.2164204865694046,\n",
       " 0.16661815345287323,\n",
       " 0.17493583261966705,\n",
       " -0.09884311258792877,\n",
       " 0.16272512078285217,\n",
       " -0.2047182023525238,\n",
       " 0.010240244679152966,\n",
       " 0.010070273652672768,\n",
       " -0.4559454917907715,\n",
       " 0.5444731116294861,\n",
       " -0.17164109647274017,\n",
       " -0.027019990608096123,\n",
       " -0.24787113070487976,\n",
       " -0.275296688079834,\n",
       " 0.15367789566516876,\n",
       " 0.04706911742687225,\n",
       " 0.11461158096790314,\n",
       " -0.42110493779182434,\n",
       " -0.18145647644996643,\n",
       " 0.2006794959306717,\n",
       " -0.1796116679906845,\n",
       " 0.2371099442243576,\n",
       " -0.4455775022506714,\n",
       " 0.052221398800611496,\n",
       " 0.35297149419784546,\n",
       " -0.10788414627313614,\n",
       " -0.0885477140545845,\n",
       " 0.24069513380527496,\n",
       " -0.06983641535043716,\n",
       " 0.06602534651756287,\n",
       " 0.05715549364686012,\n",
       " -0.003977401182055473,\n",
       " 0.1561293601989746,\n",
       " -0.27543967962265015,\n",
       " -0.22921039164066315,\n",
       " -0.2064620405435562,\n",
       " -0.010807274840772152,\n",
       " 0.22081154584884644,\n",
       " -2.2535595893859863,\n",
       " -0.32466790080070496,\n",
       " 0.19593453407287598,\n",
       " -0.4774549603462219,\n",
       " -0.34385091066360474,\n",
       " -0.24818991124629974,\n",
       " 0.28309759497642517,\n",
       " 0.26048314571380615,\n",
       " -0.05416300147771835,\n",
       " 0.10607844591140747,\n",
       " -0.1047694981098175,\n",
       " 1.93589448928833,\n",
       " -0.204649418592453,\n",
       " -0.25127679109573364,\n",
       " -0.2902164161205292,\n",
       " -0.10010039806365967,\n",
       " -0.0982394739985466,\n",
       " 0.1032903790473938,\n",
       " -0.09964233636856079,\n",
       " -0.4873983561992645,\n",
       " -0.021083667874336243,\n",
       " 1.7299293279647827,\n",
       " 0.1462237685918808,\n",
       " 0.3589695990085602,\n",
       " 0.24489162862300873,\n",
       " -0.19579291343688965,\n",
       " -0.04119151830673218,\n",
       " -0.14243081212043762,\n",
       " 0.04896533861756325,\n",
       " 0.03226412087678909,\n",
       " 0.11153113096952438,\n",
       " 0.2528059184551239,\n",
       " 0.11315745115280151]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.predict( 'yeah right' )[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on FeatureExtractionPipeline in module transformers.pipelines object:\n",
      "\n",
      "class FeatureExtractionPipeline(Pipeline)\n",
      " |  FeatureExtractionPipeline(model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Union[transformers.modelcard.ModelCard, NoneType] = None, framework: Union[str, NoneType] = None, args_parser: transformers.pipelines.ArgumentHandler = None, device: int = -1, task: str = '')\n",
      " |  \n",
      " |  Feature extraction pipeline using no model head. This pipeline extracts the hidden states from the base\n",
      " |  transformer, which can be used as features in downstream tasks.\n",
      " |  \n",
      " |  This feature extraction pipeline can currently be loaded from :func:`~transformers.pipeline` using the task\n",
      " |  identifier: :obj:`\"feature-extraction\"`.\n",
      " |  \n",
      " |  All models may be used for this pipeline. See a list of all models, including community-contributed models on\n",
      " |  `huggingface.co/models <https://huggingface.co/models>`__.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model (:obj:`~transformers.PreTrainedModel` or :obj:`~transformers.TFPreTrainedModel`):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          :class:`~transformers.PreTrainedModel` for PyTorch and :class:`~transformers.TFPreTrainedModel` for\n",
      " |          TensorFlow.\n",
      " |      tokenizer (:obj:`~transformers.PreTrainedTokenizer`):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          :class:`~transformers.PreTrainedTokenizer`.\n",
      " |      modelcard (:obj:`str` or :class:`~transformers.ModelCard`, `optional`):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (:obj:`str`, `optional`):\n",
      " |          The framework to use, either :obj:`\"pt\"` for PyTorch or :obj:`\"tf\"` for TensorFlow. The specified framework\n",
      " |          must be installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified\n",
      " |          and both frameworks are installed, will default to the framework of the :obj:`model`, or to PyTorch if no\n",
      " |          model is provided.\n",
      " |      task (:obj:`str`, defaults to :obj:`\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      args_parser (:class:`~transformers.pipelines.ArgumentHandler`, `optional`):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (:obj:`int`, `optional`, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model\n",
      " |          on the associated CUDA device id.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FeatureExtractionPipeline\n",
      " |      Pipeline\n",
      " |      _ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Extract the features of the input(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          args (:obj:`str` or :obj:`List[str]`): One or several texts (or one list of texts) to get the features of.\n",
      " |      \n",
      " |      Return:\n",
      " |          A nested list of :obj:`float`: The features computed by the model.\n",
      " |  \n",
      " |  __init__(self, model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Union[transformers.modelcard.ModelCard, NoneType] = None, framework: Union[str, NoneType] = None, args_parser: transformers.pipelines.ArgumentHandler = None, device: int = -1, task: str = '')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (:obj:`List[str]` or :obj:`dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |          pipe = pipeline(..., device=0)\n",
      " |          with pipe.device_placement():\n",
      " |              # Every framework specific tensor allocation will be done on the request device\n",
      " |              output = pipe(...)\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be :obj:`torch.Tensor`): The tensors to place on :obj:`self.device`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`Dict[str, torch.Tensor]`: The same as :obj:`inputs` but on the proper device.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_args_parser',\n",
       " '_forward',\n",
       " '_parse_and_tokenize',\n",
       " 'binary_output',\n",
       " 'check_model_type',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_tensor_on_device',\n",
       " 'framework',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'predict',\n",
       " 'return_all_scores',\n",
       " 'save_pretrained',\n",
       " 'task',\n",
       " 'tokenizer',\n",
       " 'transform']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.995046079158783},\n",
       " {'label': 'POSITIVE', 'score': 0.9995409250259399}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict([\"whatever you want\", \"Yeah right...\"] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
